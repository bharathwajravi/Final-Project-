{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d315a2e-e75b-4b3d-9094-fc9752937536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LeNet5...\n",
      "Epoch 1/10\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.1483 - accuracy: 0.9543Epoch 1/10: Loss = 0.1483, Accuracy = 0.9543\n",
      "1875/1875 [==============================] - 47s 24ms/step - loss: 0.1483 - accuracy: 0.9543 - val_loss: 0.0630 - val_accuracy: 0.9797\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0547 - accuracy: 0.9831Epoch 2/10: Loss = 0.0546, Accuracy = 0.9832\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0546 - accuracy: 0.9832 - val_loss: 0.0482 - val_accuracy: 0.9850\n",
      "Epoch 3/10\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9880Epoch 3/10: Loss = 0.0393, Accuracy = 0.9880\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0393 - accuracy: 0.9880 - val_loss: 0.0378 - val_accuracy: 0.9873\n",
      "Epoch 4/10\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0311 - accuracy: 0.9902Epoch 4/10: Loss = 0.0311, Accuracy = 0.9902\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.0311 - accuracy: 0.9902 - val_loss: 0.0462 - val_accuracy: 0.9875\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9917Epoch 5/10: Loss = 0.0258, Accuracy = 0.9917\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.0258 - accuracy: 0.9917 - val_loss: 0.0501 - val_accuracy: 0.9847\n",
      "Epoch 6/10\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9933Epoch 6/10: Loss = 0.0211, Accuracy = 0.9933\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0367 - val_accuracy: 0.9895\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9944Epoch 7/10: Loss = 0.0182, Accuracy = 0.9944\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.0324 - val_accuracy: 0.9913\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9950Epoch 8/10: Loss = 0.0152, Accuracy = 0.9950\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0456 - val_accuracy: 0.9884\n",
      "Epoch 9/10\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9960Epoch 9/10: Loss = 0.0131, Accuracy = 0.9961\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.0525 - val_accuracy: 0.9865\n",
      "Epoch 10/10\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9959Epoch 10/10: Loss = 0.0128, Accuracy = 0.9959\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.0128 - accuracy: 0.9959 - val_loss: 0.0368 - val_accuracy: 0.9901\n",
      "LeNet5 - Loss: 0.0324, Accuracy: 0.9913\n",
      "\n",
      "Training AlexNet...\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2198 - accuracy: 0.9330Epoch 1/10: Loss = 0.2198, Accuracy = 0.9330\n",
      "1875/1875 [==============================] - 5824s 3s/step - loss: 0.2198 - accuracy: 0.9330 - val_loss: 0.1008 - val_accuracy: 0.9691\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9740Epoch 2/10: Loss = 0.0950, Accuracy = 0.9740\n",
      "1875/1875 [==============================] - 6008s 3s/step - loss: 0.0950 - accuracy: 0.9740 - val_loss: 0.0590 - val_accuracy: 0.9847\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9783Epoch 3/10: Loss = 0.0809, Accuracy = 0.9783\n",
      "1875/1875 [==============================] - 6114s 3s/step - loss: 0.0809 - accuracy: 0.9783 - val_loss: 0.0584 - val_accuracy: 0.9854\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9804Epoch 4/10: Loss = 0.0726, Accuracy = 0.9804\n",
      "1875/1875 [==============================] - 5928s 3s/step - loss: 0.0726 - accuracy: 0.9804 - val_loss: 0.0743 - val_accuracy: 0.9800\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9830Epoch 5/10: Loss = 0.0651, Accuracy = 0.9830\n",
      "1875/1875 [==============================] - 5979s 3s/step - loss: 0.0651 - accuracy: 0.9830 - val_loss: 0.0570 - val_accuracy: 0.9860\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9832Epoch 6/10: Loss = 0.0647, Accuracy = 0.9832\n",
      "1875/1875 [==============================] - 5222s 3s/step - loss: 0.0647 - accuracy: 0.9832 - val_loss: 0.0384 - val_accuracy: 0.9896\n",
      "Epoch 7/10\n",
      "1773/1875 [===========================>..] - ETA: 4:29 - loss: 0.0621 - accuracy: 0.9838"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, Xception, InceptionV3\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "# Define LeNet-5\n",
    "def create_lenet5(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(6, (5, 5), activation='relu', padding='same', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(16, (5, 5), activation='relu', padding='valid'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define AlexNet\n",
    "def create_alexnet(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((3, 3), strides=2),\n",
    "        Conv2D(256, (5, 5), padding='same', activation='relu'),\n",
    "        MaxPooling2D((3, 3), strides=2),\n",
    "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        MaxPooling2D((3, 3), strides=2),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define GoogLeNet (InceptionV3)\n",
    "def create_googlenet(input_shape):\n",
    "    base_model = InceptionV3(weights='imagenet', input_shape=input_shape, include_top=False)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define VGGNet (VGG16)\n",
    "def create_vgg16(input_shape):\n",
    "    base_model = VGG16(weights='imagenet', input_shape=input_shape, include_top=False)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define ResNet50\n",
    "def create_resnet50(input_shape):\n",
    "    base_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=False)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define Xception\n",
    "def create_xception(input_shape):\n",
    "    base_model = Xception(weights='imagenet', input_shape=input_shape, include_top=False)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define SENet (using a simplified approach)\n",
    "def create_senet(input_shape):\n",
    "    base_model = Xception(weights='imagenet', input_shape=input_shape, include_top=False)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "def preprocess_image(image, target_size):\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_data(target_size, batch_size=32):\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "    train_images = np.expand_dims(train_images, axis=-1)\n",
    "    test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "    # Create TensorFlow dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "    # Preprocess the images\n",
    "    train_dataset = train_dataset.map(lambda x, y: (preprocess_image(x, target_size), y))\n",
    "    test_dataset = test_dataset.map(lambda x, y: (preprocess_image(x, target_size), y))\n",
    "\n",
    "    # Batch and shuffle\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    def one_hot_encode(images, labels):\n",
    "        labels = tf.one_hot(labels, 10)\n",
    "        return images, labels\n",
    "\n",
    "    train_dataset = train_dataset.map(one_hot_encode)\n",
    "    test_dataset = test_dataset.map(one_hot_encode)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Train and evaluate model\n",
    "def train_and_evaluate_model(model, train_dataset, test_dataset, epochs=10):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks for early stopping and model checkpoint\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "        ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    \n",
    "    # Print training progress\n",
    "    class PrintEpochProgress(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print(f\"Epoch {epoch + 1}/{self.params['epochs']}: Loss = {logs['loss']:.4f}, Accuracy = {logs['accuracy']:.4f}\")\n",
    "\n",
    "    history = model.fit(train_dataset, epochs=epochs, validation_data=test_dataset, callbacks=callbacks + [PrintEpochProgress()])\n",
    "    loss, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "    return history, loss, accuracy\n",
    "\n",
    "# Example usage\n",
    "model_params = {\n",
    "    'LeNet5': (create_lenet5, (32, 32, 3)),\n",
    "    'AlexNet': (create_alexnet, (227, 227, 3)),\n",
    "    'GoogLeNet': (create_googlenet, (299, 299, 3)),\n",
    "    'VGG16': (create_vgg16, (224, 224, 3)),\n",
    "    'ResNet50': (create_resnet50, (224, 224, 3)),\n",
    "    'Xception': (create_xception, (299, 299, 3)),\n",
    "    'SENet': (create_senet, (224, 224, 3))\n",
    "}\n",
    "\n",
    "for model_name, (create_model_fn, input_shape) in model_params.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    train_dataset, test_dataset = load_and_preprocess_data(target_size=input_shape[:2])\n",
    "    model = create_model_fn(input_shape)\n",
    "    history, loss, accuracy = train_and_evaluate_model(model, train_dataset, test_dataset, epochs=10)\n",
    "    print(f\"{model_name} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c9036-38ba-42fb-b6d1-253d869b2a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
